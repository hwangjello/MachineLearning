최적화
===================
학습모델의 매개변수 공간
----------------

>높은 차원에 비해 훈련집합의 크기가 작아 참인 확률분포를 구하는 일은 불가능하다.  
>  
>따라서 기계학습은 적절한 모델을 선택하여 목적함수를 정의하고 모델의 매개변수 공간을 탐색하여 목적함수가 최저가 되는 최적점을 찾는 전략을 사용한다.  
>  
>특징 공간에서 해야 하는 일을 모델의 매개변수 공간에서 하는 일로 대체 한 셈이다.  
><img src="https://user-images.githubusercontent.com/112842153/226810889-1a42428d-d648-48e6-a1fc-fe620d9f65d1.png" /> 
>  
>대체로 매개변수 공간은 특징 공간보다 수 배~ 수만 배 넓다.  
>예를 들어 선형 회귀(linear regression)에서는 특징 공간은 1차원, 매개변수 공간은 2차원이다.  
>  
><img src="https://user-images.githubusercontent.com/112842153/226812574-874b7dbc-7f53-4dff-9dc0-79cf9c32c4e3.png" />  
>   
>기계학습이 해야 할 일을 식으로 정의하면 위와 같은 식으로 나타낼 수 있다.  
>  
>이 때 저 식은 어떤 매개변수(세타)값에서 목적함수(J)가 최솟값을 찾는 가를 나타낸다.  
>  
최적화 문제 해결
------------------
>이러한 최적화 문제를 해결하기 위해 여러가지 알고리즘이 존재한다.  
>  
><img src="https://user-images.githubusercontent.com/112842153/226813148-6331546c-fdc0-4d72-98be-d2bdd01ade6a.png" />  
>  
>위는 낱낱탐색 알고리즘으로 차원이 조금만 높아져도 적용이 불가능한 최적화 알고리즘이다.  
>  
><img src="https://user-images.githubusercontent.com/112842153/226813346-0cc040e8-d8f6-4f68-abf0-ec9c04503066.png" />  
>  
>위는 무작위 탐색 알고리즘으로 아무런 전략이 없는 naive한 알고리즘이다.  
>  
>위 두 알고리즘은 효율이 매우 낮다. 
>  
><img src="https://user-images.githubusercontent.com/112842153/226813585-12241593-5e6b-478d-a0d0-cb6667021e58.png" />  
>  
>이 알고리즘은 기계학습이 사용하는 전형적인 알고리즘이다.  
>  
>이 때 세 번째 줄은 주로 미분을 이용해 목적함수가 작아지는 방향을 찾아내는 상황을 나타낸 것이다.  
>  
>### 미분에 의한 최적화  
>  
>f(x)의 1차 도함수 f`(x)는 원함수 f의 x에서의 기울기를 나타낸다. 
>  
>따라서 -f`(x)방향에 목적함수의 최저점이 되는 매개변수 x값이 존재한다.  
>  
>위의 알고리즘에서 역시 d@를 -f`(x)로 사용한다.  
>  
>이 것이 경사 하강 알고리즘(gradient descending algorithm)의 핵심 원리이다.  
>  